from pydantic import BaseModel, Field, RootModel  # Import RootModel
from typing import Dict, List, Optional
import json
from llama_index.core.workflow import (
    Event,
    StartEvent,
    StopEvent,
    Workflow,
    step,
)
import asyncio
from config_loader import load_shared_resources
from llama_index.llms.gemini import Gemini
from llama_index.core.workflow import Event
from llama_index.core.schema import NodeWithScore
from llama_index.core.prompts import PromptTemplate
from llama_index.core import SimpleDirectoryReader, VectorStoreIndex
from llama_index.core.workflow import (
    Context,
    Workflow,
    StartEvent,
    StopEvent,
    step,
)
from llama_index.core.schema import (
    MetadataMode,
    NodeWithScore,
    TextNode,
)
from llama_index.core.response_synthesizers import (
    ResponseMode,
    get_response_synthesizer,
)
from typing import Union, List
from llama_index.core.node_parser import SentenceSplitter
from llama_index.llms.gemini import Gemini
from redisvl.schema import IndexSchema
from config_loader import load_shared_resources
import asyncio
from llama_index.core import VectorStoreIndex, PromptTemplate
from llama_index.core.llms import ChatMessage
from llama_index.vector_stores.redis import RedisVectorStore
from redisvl.schema import IndexSchema
from llama_index.core.query_pipeline import (
    QueryPipeline,
    CustomQueryComponent
)
from llama_index.llms.gemini import Gemini
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.core.query_pipeline.components.input import InputComponent
from typing import Optional, List, Dict, Any
from pydantic import Field
from llama_index.core.schema import NodeWithScore
import traceback

config, embed_model = load_shared_resources()
llm = Gemini(
    api_key=config.get("gemini_api_key") or config.get("GEMINI_API_KEY") or config.get("GEMINI_API"), 
    model_name=config.get("llm_model")
)

def load_json_file(file_path):
    """Loads JSON data from a file."""
    try:
        with open(file_path, 'r') as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"Error: JSON file not found at '{file_path}'")
        return None
    except json.JSONDecodeError:
        print(f"Error: Could not decode JSON from file '{file_path}'. Please ensure it is valid JSON.")
        return None

class unit_abilities(RootModel):  # Back to RootModel
    """Pydantic model for Abilities within a Learning Unit."""
    root: Dict[str, str] # Root is directly a dictionary

class unit_knowledge(RootModel):  # Back to RootModel
    """Pydantic model for Knowledge within a Learning Unit."""
    root: Dict[str, str] # Root is directly a dictionary


class LearningUnitDetails(BaseModel):
    """Pydantic model for details of a single Learning Unit."""
    LO: str = Field(..., description="Learning Objective for the Learning Unit.")
    Abilities: unit_abilities = Field(..., description="Abilities associated with the Learning Unit.") # Renamed field
    Knowledge: unit_knowledge = Field(..., description="Knowledge associated with the Learning Unit.") # Renamed field
    Keywords: List[str] = Field(default_factory=list, description="Keywords for retrieval related to this Learning Unit.")
    Retrieval_Metadata: Optional[str] = Field(default=None, description="Metadata for retrieval.")
    Question: Optional[str] = Field(default=None, description="Question derived from the LU for RAG.")
    Answer: Optional[str] = Field(default=None, description="Answer generated by RAG for the Question.")

class LearningUnits(RootModel): # LearningUnits can still be RootModel if you want it to be the root dictionary
    """Pydantic model for the entire structure of Learning Units."""
    root: Dict[str, LearningUnitDetails]

# Example usage and parsing (assuming your data is in a dictionary called `lu_data`)
lu_data = load_json_file("output_json/parsed_TSC.json")

# Parse the dictionary into the Pydantic model
learning_units = LearningUnits.model_validate(lu_data) # Use .parse_obj for RootModel

class RetrieverEvent(Event):
    """Result of running retrieval"""

    nodes: list[NodeWithScore]


class CreateCitationsEvent(Event):
    """Add citations to the nodes."""

    nodes: list[NodeWithScore]

class QuestionGeneratedEvent(Event):
    """Event containing the generated question."""
    nodes: list[NodeWithScore]  # Pass the nodes forward
    question: str               # The generated question
    lu_details: LearningUnitDetails  # The learning unit details

config, embed_model = load_shared_resources()
llm = Gemini(
    api_key=config.get("gemini_api_key") or config.get("GEMINI_API_KEY") or config.get("GEMINI_API"), 
    model_name=config.get("llm_model", "gemini-1.5-pro")
)

CITATION_QA_TEMPLATE = PromptTemplate(
    "Please provide an answer based solely on the provided sources. "
    "When referencing information from a source, "
    "cite the appropriate source(s) using their corresponding numbers. "
    "Every answer should include at least one source citation. "
    "Only cite a source when you are explicitly referencing it. "
    "If none of the sources are helpful, you should indicate that. "
    "For example:\n"
    "Source 1:\n"
    "The sky is red in the evening and blue in the morning.\n"
    "Source 2:\n"
    "Water is wet when the sky is red.\n"
    "Query: When is water wet?\n"
    "Answer: Water will be wet when the sky is red [2], "
    "which occurs in the evening [1].\n"
    "Now it's your turn. Below are several numbered sources of information:"
    "\n------\n"
    "{context_str}"
    "\n------\n"
    "Query: {query_str}\n"
    "Answer: "
)

CITATION_REFINE_TEMPLATE = PromptTemplate(
    "Please provide an answer based solely on the provided sources. "
    "When referencing information from a source, "
    "cite the appropriate source(s) using their corresponding numbers. "
    "Every answer should include at least one source citation. "
    "Only cite a source when you are explicitly referencing it. "
    "If none of the sources are helpful, you should indicate that. "
    "For example:\n"
    "Source 1:\n"
    "The sky is red in the evening and blue in the morning.\n"
    "Source 2:\n"
    "Water is wet when the sky is red.\n"
    "Query: When is water wet?\n"
    "Answer: Water will be wet when the sky is red [2], "
    "which occurs in the evening [1].\n"
    "Now it's your turn. "
    "We have provided an existing answer: {existing_answer}"
    "Below are several numbered sources of information. "
    "Use them to refine the existing answer. "
    "If the provided sources are not helpful, you will repeat the existing answer."
    "\nBegin refining!"
    "\n------\n"
    "{context_msg}"
    "\n------\n"
    "Query: {query_str}\n"
    "Answer: "
)

synthesizer_prompt = """
(
    You are an expert question-answer crafter with deep domain expertise. Your task is to generate a scenario-based question and answer pair for a given knowledge statement while strictly grounding your response in the provided retrieved content. You must not hallucinate or fabricate details.

    Context information is below.\n
    ---------------------\n
    {context_str}\n
    
    Knowledge Statements:
    {knowledge_statement}\n"
    ---------------------\n
    Given the context information and not prior knowledge,
    answer the query.\n
    Query: {query_str}\n
    Guidelines:
    1. Base your response entirely on the retrieved content. If the content does not directly address the knowledge statement, do not invent new details. Instead, use minimal general context only to bridge gaps, but ensure that every key element of the final question and answer is explicitly supported by the retrieved content.
    2. Craft a realistic scenario in 2-3 sentences that reflects the context from the retrieved content while clearly addressing the given knowledge statement.
    3. Formulate one direct, simple question that ties the scenario to the knowledge statement. The question should be directly answerable using the retrieved content.
    4. Provide concise, practical bullet-point answers that list the key knowledge points explicitly mentioned in the retrieved content.         
    5. Do not mention about the source of the content in the scenario or question.
    6. Structure the final output in **valid Pydantic format**
)
"""
syn_prompt = PromptTemplate(synthesizer_prompt)

DEFAULT_CITATION_CHUNK_SIZE = 512
DEFAULT_CITATION_CHUNK_OVERLAP = 20

def define_custom_schema():
    """Defines the custom Redis index schema."""
    return IndexSchema.from_dict(
        {
            "index": {"name": "redis_vector_store", "prefix": "doc"},
            "fields": [
                {"type": "tag", "name": "id"},
                {"type": "tag", "name": "doc_id"},
                {"type": "text", "name": "text"},
                {"type": "numeric", "name": "updated_at"},
                {"type": "tag", "name": "file_name"},
                {
                    "type": "vector",
                    "name": "vector",
                    "attrs": {
                        "dims": 384,
                        "algorithm": "hnsw",
                        "distance_metric": "cosine",
                    },
                },
            ],
        }
    )

class LUStopEvent(StopEvent):
    """Custom StopEvent that includes Learning Unit Details."""
    
    lu_details: LearningUnitDetails = None
    
    def __init__(self, result: Any = None, lu_details: LearningUnitDetails = None, **kwargs: Any) -> None:
        super().__init__(result=result, **kwargs)
        self.lu_details = lu_details
    
    def _get_result(self) -> Any:
        """Override to return desired result."""
        return self._result

# Define a custom Event class to hold Learning Unit data
class LUDataEvent(Event):
    lu_details: LearningUnitDetails
    query: Optional[List[str]] = None
    index: Optional[Any] = None

class PydanticWorkflow(Workflow):
    llm = llm # Initialize LLM in the Workflow class

    @step
    async def keyword_generation(self, ev: StartEvent) -> LUDataEvent: # First step takes StartEvent
        lu_details = ev.lu_details # Access lu_details from StartEvent (passed from main())

        # LLM Keyword Generation Prompt
        prompt_keywords = f"""Generate 5-7 keywords that are highly relevant to the following Learning Unit.
        Keywords should be concise and capture the core concepts.

        Learning Objective: {lu_details.LO}
        Abilities: {lu_details.Abilities.root}
        Knowledge: {lu_details.Knowledge.root}

        Keywords:"""

        # Call LLM to generate keywords
        response = await self.llm.acomplete(prompt_keywords) # Use await for async LLM call
        generated_keywords = response.text.strip().split(", ") # Simple split, refine as needed

        # Update LearningUnitDetails with generated keywords
        lu_details.Keywords = generated_keywords # Update the Keywords list

        print(f"Generated Keywords for {lu_details.Keywords}") # Print generated keywords
        query = lu_details.Keywords
        # First step now *receives*, *updates*, and packages the data into LUDataEvent
        return LUDataEvent(lu_details=lu_details, query=query, index=ev.index)

    @step
    async def retrieve(
        self, ctx: Context, ev: LUDataEvent
    ) -> Union[RetrieverEvent, None]:
        "Entry point for RAG, triggered by a LUDataEvent with `query`."
        query = ev.query
        if not query:
            return None
        
        print(f"Query the database with: {query}")
        
        # store the query in the global context
        await ctx.set("query", query)
        # Store lu_details in context for later use
        await ctx.set("lu_details", ev.lu_details)
        
        index = ev.index
        if index is None:
            print("Index is empty, load some documents before querying!")
            return None
            
        retriever = index.as_retriever(similarity_top_k=2)
        # Join the keywords into a query string
        query_str = " ".join(query) if isinstance(query, list) else query
        nodes = retriever.retrieve(query_str)
        print(f"Retrieved {len(nodes)} nodes.")
        return RetrieverEvent(nodes=nodes)

    @step
    async def create_citation_nodes(
        self, ev: RetrieverEvent
    ) -> CreateCitationsEvent:
        """
        Modify retrieved nodes to create granular sources for citations.

        Takes a list of NodeWithScore objects and splits their content
        into smaller chunks, creating new NodeWithScore objects for each chunk.
        Each new node is labeled as a numbered source, allowing for more precise
        citation in query results.

        Args:
            nodes (List[NodeWithScore]): A list of NodeWithScore objects to be processed.

        Returns:
            List[NodeWithScore]: A new list of NodeWithScore objects, where each object
            represents a smaller chunk of the original nodes, labeled as a source.
        """
        nodes = ev.nodes

        new_nodes: List[NodeWithScore] = []

        text_splitter = SentenceSplitter(
            chunk_size=DEFAULT_CITATION_CHUNK_SIZE,
            chunk_overlap=DEFAULT_CITATION_CHUNK_OVERLAP,
        )

        for node in nodes:
            text_chunks = text_splitter.split_text(
                node.node.get_content(metadata_mode=MetadataMode.NONE)
            )

            for text_chunk in text_chunks:
                text = f"Source {len(new_nodes)+1}:\n{text_chunk}\n"

                new_node = NodeWithScore(
                    node=TextNode.model_validate(node.node), score=node.score
                )
                new_node.node.text = text
                new_nodes.append(new_node)
                print(f"New Node: {new_node.node.text}")
        return CreateCitationsEvent(nodes=new_nodes)

    @step
    async def synthesize(self, ctx: Context, ev: CreateCitationsEvent) -> StopEvent:
        """Return a streaming response using the retrieved nodes."""
        try:
            query = await ctx.get("query", default=None)
            lu_details = await ctx.get("lu_details", default=None)
            print(lu_details)
            
            # Convert list to string if needed
            if isinstance(query, list):
                query_str = "Based on the retrieved context, create a question and answer that takes into account the additional knowledge statements above."
            else:
                query_str = query
            
            synthesizer = get_response_synthesizer(
                llm=llm,
                text_qa_template=CITATION_QA_TEMPLATE,
                refine_template=CITATION_REFINE_TEMPLATE,
                response_mode=ResponseMode.REFINE,            
                use_async=True,
            )

            # Use query_str instead of query
            response = await synthesizer.asynthesize(query_str, nodes=ev.nodes)
            response_text = response.response
            
            # Parse the response to extract question and answer
            if "Question:" in response_text and "Answer:" in response_text:
                # Split by "Answer:" to get question part and answer part
                parts = response_text.split("Answer:", 1)
                
                if len(parts) == 2:
                    # Extract question (remove "Question:" prefix and trim)
                    question_part = parts[0].replace("Question:", "", 1).strip()
                    # Extract answer (trim)
                    answer_part = parts[1].strip()
                    
                    # Update the lu_details with the separated question and answer
                    if lu_details:
                        lu_details.Question = question_part
                        lu_details.Answer = answer_part
                        print(f"Extracted Question: {question_part}")
                        print(f"Extracted Answer: {answer_part}")
                else:
                    # Fallback if parsing fails
                    if lu_details:
                        lu_details.Question = "Could not parse question"
                        lu_details.Answer = response_text
            else:
                # Fallback if the response format is unexpected
                if lu_details:
                    lu_details.Question = "No question found in response"
                    lu_details.Answer = response_text
                    
            # Clear context variables to help with cleanup
            await ctx.clear()
                    
            return LUStopEvent(result=response, lu_details=lu_details)
        except Exception as e:
            print(f"Error in synthesize step: {str(e)}")
            # Still try to clean up and return 
            await ctx.clear()
            traceback.print_exc()
            return LUStopEvent(result=f"Error: {str(e)}")

# left with synthesizer, decide on whether to use autogen groupchats
# simple llm calls with pydantic output
# or attempt to work with llamaindex synthesizers (seems limited in ability from testing so far)
# or use a custom synthesizer
# or use a custom synthesizer with a custom synthesizer prompt

async def main():
    # Load data *once* outside the workflow loop
    lu_data = load_json_file("output_json/parsed_TSC.json")
    learning_units = LearningUnits.model_validate(lu_data)
    
    # Define the Redis schema
    custom_schema = define_custom_schema()

    # Initialize the RedisVectorStore
    vector_store = RedisVectorStore(
        schema=custom_schema, redis_url="redis://localhost:6379"
    )

    # Load the index from the existing Redis store
    index = VectorStoreIndex.from_vector_store(
        vector_store, embed_model=embed_model
    )
    
    # Process each learning unit with better isolation
    processed_units = {}  # Store processed results here
    
    # Loop through each Learning Unit
    for lu_name, lu_details in learning_units.root.items():
        print(f"\n==== Starting workflow for LU: {lu_name} ====")
        
        try:
            # Create new workflow in a more isolated way
            w = PydanticWorkflow(timeout=300, verbose=True)
            
            # Process this LU in a controlled scope
            result = await w.run(lu_details=lu_details, index=index)
            
            # Extract the important data
            processed_lu_details = result.lu_details if hasattr(result, 'lu_details') else None
            
            # Store successful results
            if processed_lu_details:
                processed_units[lu_name] = processed_lu_details
                print(f"Successfully processed: {lu_name}")
                
                if hasattr(result, 'result'):
                    print(f"Response: {result.result.response}")
            
            # Force garbage collection of the workflow
            del w
            
            # Add a small delay to ensure resources are released
            await asyncio.sleep(1)
            
        except Exception as e:
            print(f"Error processing {lu_name}: {str(e)}")
            traceback.print_exc()
            continue
    
    # After all LUs are processed, update the learning_units object
    for lu_name, processed_details in processed_units.items():
        learning_units.root[lu_name] = processed_details
    
    # Save the updated data
    try:
        with open("output_json/updated_TSC.json", "w") as f:
            json.dump(learning_units.model_dump(), f, indent=2)
        print("Successfully saved updated learning units to file")
    except Exception as e:
        print(f"Error saving updated data: {str(e)}")

    print(f"Processed {len(processed_units)}/{len(learning_units.root)} learning units")

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())